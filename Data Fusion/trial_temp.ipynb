{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.independencies import Independencies\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "import dowhy.gcm as gcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAG describing this data generating process:\n",
    "G = nx.DiGraph()\n",
    "# G.add_edges_from([\n",
    "#     ('A', 'G'),\n",
    "#     ('B', 'G'),\n",
    "#     ('G', 'H'),\n",
    "#     ('C', 'F'),\n",
    "#     ('D', 'F'),\n",
    "#     ('E', 'F'),\n",
    "#     ('F', 'H'),\n",
    "#     ('B', 'H'),\n",
    "# ])\n",
    "\n",
    "G.add_edges_from([\n",
    "    ('X', 'A'),\n",
    "    ('g', 'A'),\n",
    "    ('g', 'B'),\n",
    "    ('Y', 'B'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Data Generation\n",
    "# def data_gen_process(seed=42, n_samples=1000):\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "#     A = np.random.binomial(1, 0.5, n_samples)\n",
    "#     B = np.random.normal(0, 1, n_samples)\n",
    "#     C = np.random.normal(0, 1, n_samples)\n",
    "#     D = np.random.normal(0, 1, n_samples)\n",
    "#     E = np.random.normal(0, 1, n_samples)\n",
    "\n",
    "#     G = np.tanh(A + 0.5 * B) + np.random.normal(0, 0.5, n_samples)  # Increase noise\n",
    "#     F = np.sin(C) + np.log(np.abs(D) + 1) + E**2 + np.random.normal(0, 0.5, n_samples)  # Increase noise\n",
    "\n",
    "#     H_base = G**2 + 2 * np.cos(F) + 0.5 * B**2 + np.random.normal(0, 0.5, n_samples)  # More complex non-linear relationship\n",
    "#     H = np.where(B > 0, H_base - 1.5 * G, H_base + 1.5 * G)  # Simpson's effect\n",
    "\n",
    "#     Z = np.random.binomial(1, 0.5, n_samples)\n",
    "#     H += Z * np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "#     # Discretize H\n",
    "#     discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "#     H_discrete = discretizer.fit_transform(H.reshape(-1, 1)).flatten()\n",
    "\n",
    "#     return pd.DataFrame({\n",
    "#         'A': A,\n",
    "#         'B': B,\n",
    "#         'C': C,\n",
    "#         'D': D,\n",
    "#         'E': E,\n",
    "#         'G': G,\n",
    "#         'F': F,\n",
    "#         'H': H_discrete,\n",
    "#         'Z': Z \n",
    "#     })\n",
    "\n",
    "# df = data_gen_process()\n",
    "# # Preprocess the data to include only the relevant variables based on the DAG\n",
    "# relevant_variables = ['A', 'B', 'C', 'D', 'E', 'G', 'F', 'Z']\n",
    "# X = df[relevant_variables]\n",
    "# y = df['H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "def data_gen_process(seed=42, n_samples=1000):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    g = np.random.normal(1, 0.5, n_samples)\n",
    "    X = np.random.normal(0, 1, n_samples)\n",
    "    Y = np.random.normal(0, 1, n_samples)\n",
    "    A = np.tanh(X + 0.5 * g) + np.random.normal(0, 0.5, n_samples)\n",
    "    B = np.sin(g) + np.log(np.abs(g) + 1) + Y**2 + np.random.normal(0, 0.5, n_samples)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'g': g,\n",
    "        'X': X,\n",
    "        'Y': Y,\n",
    "        'A': A,\n",
    "        'B': B,\n",
    "    })\n",
    "\n",
    "df = data_gen_process()\n",
    "# Preprocess the data to include only the relevant variables based on the DAG\n",
    "relevant_variables = ['X', 'Y', 'A'] # 'g' hidden\n",
    "X = df[relevant_variables]\n",
    "y = df['B']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cond Indep Test\n",
    "(using the kernel dependence measure?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A âŸ‚ Y) : frozenset({'A'}) frozenset({'Y'}) frozenset()\n"
     ]
    }
   ],
   "source": [
    "model = BayesianModel(list(G.edges()))\n",
    "independencies = model.get_independencies() # can reduce a lot of redundancies\n",
    "for indep in independencies.get_assertions():\n",
    "    print(indep, \":\", indep.event1, indep.event2, indep.event3)\n",
    "    # print(gcm.independence_test(df[indep.event1].to_numpy(), \n",
    "    #                       df[indep.event2].to_numpy(), \n",
    "    #                       conditioned_on=df[indep.event3].to_numpy()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/latex": [
       "$\\displaystyle 0.834782684893678$"
      ],
      "text/plain": [
       "0.8347826848936777"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gcm.independence_test(df.A.to_numpy(), df.Y.to_numpy())\n",
    "# gcm.independence_test(np.random.normal(1, 2, 1000000), np.random.normal(0, 1, 1000000))\n",
    "gcm.independence_test(np.random.poisson(1, 1000000), np.random.normal(1, 2, 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianModel(list(G.edges()))\n",
    "independencies = model.get_independencies() # can reduce a lot of redundancies\n",
    "\n",
    "indep_constraints_status = []\n",
    "for indep in independencies.get_assertions():\n",
    "    val = gcm.independence_test(df[indep.event1].to_numpy(), \n",
    "                          df[indep.event2].to_numpy(), \n",
    "                          conditioned_on=df[indep.event3].to_numpy())\n",
    "    print(indep, val)\n",
    "    indep_constraints_status.append([indep,val])\n",
    "\n",
    "indep_constraints_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalty Calculation (Loss func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_penalty(predictions, variables, states):\n",
    "    penalty = 0\n",
    "    \n",
    "    # Example calculation for sum-to-one constraint\n",
    "    for node in variables:\n",
    "        sum_probs = tf.reduce_sum([predictions[node][state] for state in states[node]], axis=0)\n",
    "        penalty += tf.reduce_mean(tf.square(sum_probs - 1))  # Sum-to-one constraint penalty\n",
    "    \n",
    "    # Example calculation for non-negativity constraint\n",
    "    for node in variables:\n",
    "        for state in states[node]:\n",
    "            penalty += tf.reduce_mean(tf.nn.relu(-predictions[node][state]))  # Non-negativity constraint penalty\n",
    "    \n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_penalty(predictions, X, model):   \n",
    "    constraints_penalty = 0\n",
    "\n",
    "    # Conditional Independences\n",
    "    independencies = model.get_independencies()\n",
    "    for independence in independencies.get_assertions():\n",
    "        var1 = independence.event1\n",
    "        var2 = independence.event2\n",
    "        ...\n",
    "\n",
    "    # Sum-to-One Constraints\n",
    "    if np.any(np.abs(np.sum(predictions, axis=1) - 1) > 1e-5):\n",
    "        constraints_penalty += np.sum(np.abs(np.sum(predictions, axis=1) - 1))\n",
    "\n",
    "    # Non-negativity Constraints\n",
    "    if np.any(predictions < 0):\n",
    "        constraints_penalty += np.sum(predictions[predictions < 0]**2)\n",
    "\n",
    "    return constraints_penalty\n",
    "\n",
    "# Custom loss function to include the DAG-induced constraints\n",
    "def custom_loss(y_true, y_pred):\n",
    "    base_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    penalty = calculate_penalty(y_pred, X, BayesianModel(list(G.edges())))    \n",
    "    return base_loss + penalty\n",
    "\n",
    "# Define the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, input_dim=X.shape[1], activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(X, y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_penalty(predictions, variables, states):   \n",
    "    model = BayesianModel(list(G.edges()))\n",
    "    constraints = []\n",
    "\n",
    "    # Conditional Independences\n",
    "    independencies = model.get_independencies()\n",
    "    # for independence in independencies.get_assertions():\n",
    "    #     constraints.append(f\"{independence}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Sum-to-One Constraints\n",
    "    # ...\n",
    "       \n",
    "    # Non-negativity Constraints\n",
    "    # ...\n",
    "\n",
    "\n",
    "    # return constraints\n",
    "    for constraint in constraints:\n",
    "        print(constraint)\n",
    "\n",
    "    penalty = ...\n",
    "\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function to include the DAG-induced constraints\n",
    "def custom_loss(y_true, y_pred):\n",
    "    base_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    penalty = calculate_penalty(predictions, variables, states)\n",
    "    return base_loss + penalty\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=X.shape[1], activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Train the model #bayesian hyperparameter optimization?\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "print(\"Done fitting.\")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to identify DAG induced constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_constraints(model):\n",
    "\n",
    "\n",
    "    constraints = []\n",
    "    \n",
    "    # Conditional Independences\n",
    "    independencies = model.get_independencies()\n",
    "    for independence in independencies.get_assertions():\n",
    "        constraints.append(f\"{independence}\")\n",
    "    \n",
    "    # # Sum-to-One Constraints\n",
    "    # for node in model.nodes():\n",
    "    #     states = ['x1', 'x2', 'x3']  # Example states, should be defined based on your data\n",
    "    #     sum_to_one = ' + '.join([f'P({node}={state})' for state in states]) + ' = 1'\n",
    "    #     constraints.append(sum_to_one)\n",
    "    \n",
    "    # # Non-negativity Constraints\n",
    "    # for node in model.nodes():\n",
    "    #     states = ['x1', 'x2', 'x3']  # Example states, should be defined based on your data\n",
    "    #     for state in states:\n",
    "    #         non_negativity = f'P({node}={state}) >= 0'\n",
    "    #         constraints.append(non_negativity)\n",
    "    \n",
    "    return constraints\n",
    "\n",
    "# Example usage\n",
    "constraints = generate_constraints(model)\n",
    "for constraint in constraints:\n",
    "    print(constraint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianModel(list(G.edges()))\n",
    "\n",
    "\n",
    "# conditional independences\n",
    "independencies = model.get_independencies()\n",
    "print(independencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: If you have a distribution P(X)\n",
    "# The sum-to-one constraint for a discrete variable X with states {x1, x2, x3} is:\n",
    "sum_to_one_constraint = 'P(x1) + P(x2) + P(x3) = 1'\n",
    "print(sum_to_one_constraint)\n",
    "\n",
    "\n",
    "# Example: For a probability P(X=x)\n",
    "non_negativity_constraints = ['P(x) >= 0']\n",
    "print(non_negativity_constraints)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
