{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Interventions and Counterfactuals\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to work with either categorical or continues variables, want to specify whether monotiniciy/markovianity is needed, want to either specify loss function or make it clear that post-processing threshold adjustment is needed instead, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('renewal_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to discretisize data:\n",
    "# split discretised_data[\"interactions\"] to 5 bins\n",
    "discretised_data = df.copy()\n",
    "discretised_data[\"interactions\"] = pd.qcut(discretised_data[\"interactions\"], 5, labels=False)\n",
    "\n",
    "# train test validate\n",
    "train, test = train_test_split(discretised_data, train_size=0.75, test_size=0.25, random_state=7)\n",
    "# ways to include validation:\n",
    "# train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "# X_train, X_test, y_train, y_test  = train_test_split(discretised_data[...], discretised_data['renewal'], test_size=0.2, random_state=1)\n",
    "# X_train, X_val, y_train, y_val  = train_test_split(discretised_data[...], discretised_data['renewal'], test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods:\n",
    "(1) MLE (causalnex, dowhy, causal-learn, etc)  \n",
    "(2) Bayesian Estimator  \n",
    "(3) TARNET  \n",
    "(4) ...  \n",
    "\n",
    "Want neural network method to learn complexity without assuming specific functional relationship, except maybe monotonicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gurobi method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('renewal_data.csv')\n",
    "\n",
    "\n",
    "# # estimating P(A|x, do(y))...\n",
    "# from sem_estimation import sem_estimator\n",
    "\n",
    "\n",
    "# # [parent, child]\n",
    "# edges = [[\"X\", \"A\"], [\"l\", \"A\"], [\"l\", \"B\"], [\"Y\", \"B\"]] # edges = [[\"Z\", \"A\"], [\"l\", \"A\"], [\"l\", \"B\"]]\n",
    "# cards_dict = {\"A\": 2, \"B\": 2, \"X\": 2, \"Y\": 2, \"l\": 3} # cards_dict = {\"A\": 2, \"B\": 2, \"Z\": 2, \"l\": 3}\n",
    "# hiddens_lst = [\"l\"]\n",
    "\n",
    "# example = sem_estimator(edges, hiddens_lst, cards_dict)\n",
    "# # example.initialize_model()\n",
    "# # example.get_joint()\n",
    "# example.specify_loss(\"MLE\")\n",
    "# example.fit_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from interuption_trial_done import P_YdoXs\n",
    "# P_YdoXs(1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do-Why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dowhy import gcm\n",
    "import numpy as np, pandas as pd\n",
    "import networkx as nx\n",
    "# from scipy.stats import norm, binom, poisson, beta, expon\n",
    "import CausalEGM as cegm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges = [('is_senior', 'interactions'),\n",
    "#          ('sale_calls', 'interactions'),\n",
    "#          ('interactions', 'renewal'),\n",
    "#          ('bugs_faced', 'renewal'),\n",
    "#          ('bugs_faced', 'discount'),\n",
    "#          ('monthly_usage', 'renewal'),\n",
    "#          ('monthly_usage', 'discount'),\n",
    "#          ('consumer_trust', 'renewal'),\n",
    "#          ('discount', 'renewal')]\n",
    "# causal_model = gcm.ProbabilisticCausalModel(nx.DiGraph(edges))\n",
    "\n",
    "# # define orphan nodes distribution:\n",
    "# causal_model.set_causal_mechanism('is_senior', gcm.ScipyDistribution(binom))\n",
    "# causal_model.set_causal_mechanism('sale_calls', gcm.ScipyDistribution(poisson))\n",
    "# causal_model.set_causal_mechanism('bugs_faced', gcm.ScipyDistribution(poisson))\n",
    "# causal_model.set_causal_mechanism('monthly_usage', gcm.ScipyDistribution(expon))\n",
    "# causal_model.set_causal_mechanism('consumer_trust', gcm.ScipyDistribution(beta))\n",
    "\n",
    "# # define dependent nodes distribution:\n",
    "# causal_model.set_causal_mechanism(\n",
    "#     'renewal',\n",
    "#     gcm.AdditiveNoiseModel(\n",
    "#         prediction_model=gcm.ml.create_linear_regressor(),\n",
    "#         noise_model=gcm.ScipyDistribution(norm)))\n",
    "\n",
    "\n",
    "# gcm.fit(causal_model, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CausalEGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "params = yaml.safe_load(open('../../src/configs/Semi_acic.yaml', 'r'))\n",
    "print(params)\n",
    "model = cegm.CausalEGM(params=params,random_seed=123)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
