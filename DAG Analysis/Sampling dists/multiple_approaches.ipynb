{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # Latent variable gamma\n",
    "    gamma = pm.Bernoulli('gamma', p=0.5)\n",
    "    \n",
    "    # Input variables X and Y\n",
    "    X = pm.Bernoulli('X', p=0.5)\n",
    "    Y = pm.Bernoulli('Y', p=0.5)\n",
    "    \n",
    "    # Conditional probabilities for A and B given X, Y, gamma\n",
    "    A = pm.Bernoulli('A', p=pm.math.switch(gamma, pm.math.switch(X, 0.9, 0.1), pm.math.switch(X, 0.2, 0.8)))\n",
    "    B = pm.Bernoulli('B', p=pm.math.switch(gamma, pm.math.switch(Y, 0.9, 0.1), pm.math.switch(Y, 0.2, 0.8)))\n",
    "    \n",
    "    # Sample from the model\n",
    "    trace = pm.sample(1000)\n",
    "\n",
    "# Accessing the posterior samples\n",
    "print(az.summary(trace, var_names=['A', 'B', 'X', 'Y']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace is the InferenceData object from sampling\n",
    "\n",
    "# Extract the samples\n",
    "samples = az.extract_dataset(trace, group='posterior', var_names=['A', 'B', 'X', 'Y'])\n",
    "A_samples = samples.A.values.flatten()\n",
    "B_samples = samples.B.values.flatten()\n",
    "X_samples = samples.X.values.flatten()\n",
    "Y_samples = samples.Y.values.flatten()\n",
    "\n",
    "# Combine the samples into a single array of tuples\n",
    "data = list(zip(A_samples, B_samples, X_samples, Y_samples))\n",
    "\n",
    "# Create a dictionary to count occurrences of each (A, B, X, Y) combination\n",
    "joint_distribution = {}\n",
    "\n",
    "for entry in data:\n",
    "    if entry in joint_distribution:\n",
    "        joint_distribution[entry] += 1\n",
    "    else:\n",
    "        joint_distribution[entry] = 1\n",
    "\n",
    "# Convert counts to probabilities by dividing by the total number of samples\n",
    "total_samples = len(data)\n",
    "for key in joint_distribution:\n",
    "    joint_distribution[key] /= total_samples\n",
    "\n",
    "# Print the joint distribution\n",
    "print(joint_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract samples from the trace\n",
    "A_samples = trace.posterior['A'].values.flatten()\n",
    "B_samples = trace.posterior['B'].values.flatten()\n",
    "X_samples = trace.posterior['X'].values.flatten()\n",
    "Y_samples = trace.posterior['Y'].values.flatten()\n",
    "\n",
    "# Stack them into a single array for easier processing\n",
    "samples = np.vstack((A_samples, B_samples, X_samples, Y_samples)).T\n",
    "\n",
    "# Find unique rows and their counts\n",
    "unique_rows, counts = np.unique(samples, axis=0, return_counts=True)\n",
    "\n",
    "# Calculate probabilities by normalizing counts\n",
    "probabilities = counts / counts.sum()\n",
    "\n",
    "# Create a dictionary to store the joint distribution\n",
    "joint_distribution = {}\n",
    "for i, row in enumerate(unique_rows):\n",
    "    key = tuple(row)  # Create a tuple key (A, B, X, Y)\n",
    "    joint_distribution[key] = probabilities[i]\n",
    "\n",
    "# Output the joint distribution\n",
    "print(joint_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "import numpy as np\n",
    "\n",
    "model = gp.Model(\"\")\n",
    "\n",
    "cardA, cardB, cardX, cardY, cardL = 2, 2, 2, 2, 8\n",
    "P_ABXY = model.addMVar(shape=(cardA, cardB, cardX, cardY), vtype=gp.GRB.CONTINUOUS, lb=0, ub=1, name='P_ABXY')\n",
    "\n",
    "P_A = P_ABXY.sum(axis=(1, 2, 3))\n",
    "P_B = P_ABXY.sum(axis=(0, 2, 3))\n",
    "P_X = np.array([1/cardA for _ in range(cardA)]) # uniform distribution across setting options\n",
    "P_Y = np.array([1/cardA for _ in range(cardA)]) # uniform distribution across setting options\n",
    "\n",
    "##### creating a random distribution for P_l with given cardinality ######\n",
    "rand_numbs = np.random.random(cardL)\n",
    "normd_numbs = rand_numbs / np.sum(rand_numbs)\n",
    "scaled_numbs = np.round(normd_numbs * 10**3) / 10**3\n",
    "correction = 1 - np.sum(scaled_numbs)\n",
    "largest_index = np.argmax(scaled_numbs)\n",
    "scaled_numbs[largest_index] += correction\n",
    "\n",
    "P_l = scaled_numbs\n",
    "######\n",
    "\n",
    "\n",
    "# NOT HOW THIS SHOULD BE DONE\n",
    "P_A_giv_Xl = model.addMVar(shape=(cardA, cardX, cardL), vtype=gp.GRB.CONTINUOUS, lb=0, ub=1, name='P(A|Xl)')\n",
    "P_B_giv_Yl = model.addMVar(shape=(cardB, cardY, cardL), vtype=gp.GRB.CONTINUOUS, lb=0, ub=1, name='P(B_Yl)')\n",
    "model.update()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Define necessary conditional probabilities\n",
    "# P_A_given_X = np.zeros((cardA, cardX))\n",
    "# P_B_given_Y = np.zeros((cardB, cardY))\n",
    "# P_Y_given_X = np.zeros((cardY, cardX))\n",
    "# P_X_given_Y = np.zeros((cardX, cardY))\n",
    "# P_X_given_A = np.zeros((cardA, cardX))\n",
    "# P_Y_given_A = np.zeros((cardA, cardY))\n",
    "# P_X_given_B = np.zeros((cardB, cardX))\n",
    "# P_Y_given_B = np.zeros((cardB, cardY))\n",
    "# for var1, var2 in np.ndindex(2,2):\n",
    "#     P_A_given_X[var1, var2] = P_ABXY[var1, :, var2, :].sum() / P_X[var2]\n",
    "#     P_B_given_Y[var1, var2] = P_ABXY[:, var1, :, var2].sum() / P_Y[var2]\n",
    "#     P_Y_given_X[var1, var2] = P_ABXY[:, :, var2, var1].sum() / P_X[var2]\n",
    "#     P_X_given_Y[var1, var2] = P_ABXY[:, :, var1, var2].sum() / P_Y[var2]\n",
    "#     P_X_given_A[var1, var2] = P_ABXY[var2, :, var1, :].sum() / P_A[var2]\n",
    "#     P_Y_given_A[var1, var2] = P_ABXY[var2, :, :, var1].sum() / P_A[var2]\n",
    "#     P_X_given_B[var1, var2] = P_ABXY[:, var2, var1, :].sum() / P_B[var2]\n",
    "#     P_Y_given_B[var1, var2] = P_ABXY[:, var2, :, var1].sum() / P_B[var2]\n",
    "def P_A_given_X(a, x):\n",
    "    return P_ABXY[a, :, x, :].sum() / P_X[x]\n",
    "def P_B_given_Y(b, y):\n",
    "    return P_ABXY[:, b, :, y].sum() / P_Y[y]\n",
    "def P_Y_given_X(y, x):\n",
    "    return P_ABXY[:, :, x, y].sum() / P_X[x]\n",
    "def P_X_given_Y(x, y):\n",
    "    return P_ABXY[:, :, x, y].sum() / P_Y[y]\n",
    "def P_X_given_A(x, a):\n",
    "    return P_ABXY[a, :, x, :].sum() / P_A[a]\n",
    "def P_Y_given_A(y, a):\n",
    "    return P_ABXY[a, :, :, y].sum() / P_A[a]\n",
    "def P_X_given_B(x, b):\n",
    "    return P_ABXY[:, b, x, :].sum() / P_B[b]\n",
    "def P_Y_given_B(y, b):\n",
    "    return P_ABXY[:, b, :, y].sum() / P_B[b]\n",
    "\"\"\"\n",
    "for x,a in np.ndindex(cardX, cardA):\n",
    "    P_A_given_X[a, x] = P_ABXY[a, :, x, :].sum() / P_X[x]\n",
    "for y,b in np.ndindex(cardY, cardB):\n",
    "    P_B_given_Y[b, y] = P_ABXY[:, b, :, y].sum() / P_Y[y]\n",
    "for x,y in np.ndindex(cardX, cardY):\n",
    "    P_Y_given_X[y, x] = P_ABXY[:, :, x, y].sum() / P_X[x]\n",
    "for y,x in np.ndindex(cardY, cardX):\n",
    "    P_X_given_Y[x, y] = P_ABXY[:, :, x, y].sum() / P_Y[y]\n",
    "for a,x in np.ndindex(cardA, cardX):\n",
    "    P_X_given_A[a, x] = P_ABXY[a, :, x, :].sum() / P_A[a]\n",
    "for a,y in np.ndindex(cardA, cardY):\n",
    "    P_Y_given_A[a, y] = P_ABXY[a, :, :, y].sum() / P_A[a]\n",
    "for b,x in np.ndindex(cardB, cardX):\n",
    "    P_X_given_B[b, x] = P_ABXY[:, b, x, :].sum() / P_B[b]\n",
    "for b,y in np.ndindex(cardB, cardY):\n",
    "    P_Y_given_B[b, y] = P_ABXY[:, b, :, y].sum() / P_B[b]\n",
    "\"\"\"\n",
    "\n",
    "# SEM\n",
    "for a,b,x,y in np.ndindex(cardA, cardB, cardX, cardY):\n",
    "    # model.addConstr(P_AB_giv_XY[a,b,x,y] == [P_A_giv_Xl[a,x,l] * P_B_giv_Yl(b,y,l)*P_l[l] for l in range(cardL)].sum())\n",
    "    model.addConstr(P_ABXY[a,b,x,y] == sum([P_A_giv_Xl[a,x,l] * P_B_giv_Yl[b,y,l]*P_X[x]*P_Y[y]*P_l[l] for l in range(cardL)]))\n",
    "\n",
    "\n",
    "# conditional independence constraints for bell DAG\n",
    "\n",
    "# X ⫫ Y \n",
    "# -> P(X, Y) = P(X)P(Y)\n",
    "for x,y in np.ndindex(cardX, cardY):\n",
    "    model.addConstr(P_ABXY[:, :, x, y].sum() == P_X[x] * P_Y[y], name='X⫫Y')\n",
    "    \n",
    "# A ⫫ Y | X \n",
    "# -> P(A, Y | X) = P(A | X)P(Y | X)\n",
    "for a,x,y in np.ndindex(cardA, cardX, cardY):\n",
    "    model.addConstr(\n",
    "        # P(A,X,Y)/P(X) = P(A,Y|X) = P(A|X)P(Y|X)\n",
    "        P_ABXY[a, :, x, y].sum() / P_X[x] == P_A_given_X[a, x] * P_Y_given_X[y, x], name=f'P(A,Y|X)=P(A|X)P(Y|X)')\n",
    "\n",
    "# B ⫫ X | Y \n",
    "# -> P(B, X | Y) = P(B | Y)P(X | Y)\n",
    "for b,x,y in np.ndindex(cardB,cardX,cardY):\n",
    "    model.addConstr(\n",
    "        # P(B,X,Y)/P(Y) = P(B,X|Y) = P(B|Y)P(X|Y)\n",
    "        P_ABXY[:, b, x, y].sum() / P_Y[y] == P_B_given_Y[b, y] * P_X_given_Y[x, y], name=f'P(B,X|Y)=P(B|Y)P(X|Y)')\n",
    "\n",
    "# A ⫫ Y \n",
    "# -> P(A, Y) = P(A)P(Y)\n",
    "for a,y in np.ndindex(cardA, cardY):\n",
    "    model.addConstr(P_ABXY[a, :, :, y].sum() == P_A[a] * P_Y[y], name=f'A⫫Y_{a}_{y}')\n",
    "\n",
    "# B ⫫ X \n",
    "# -> P(B,X) = P(B)P(X)\n",
    "for b,x in np.ndindex(cardB, cardX):\n",
    "    model.addConstr(P_ABXY[:, b, x, :].sum() == P_B[b] * P_X[x], name=f'B⫫X_{b}_{x}')\n",
    "\n",
    "for a,x,y in np.ndindex(cardA, cardX, cardY):\n",
    "    joint_prob = P_ABXY[a, :, x, y].sum() / P_A[a] # P(X,Y|A) = P(X|A)P(Y|A)\n",
    "    model.addConstr(joint_prob == P_X_given_A[a, x] * P_Y_given_A[a, y], name=f'P(X,Y|A) = P(X|A)P(Y|A)')\n",
    "\n",
    "# X ⫫ Y | B -> P(X, Y | B) = P(X | B)P(Y | B) and = P(X)P(Y | B)\n",
    "for b,x,y in np.ndindex(cardB, cardX, cardY):\n",
    "    joint_prob = P_ABXY[:, b, x, y].sum() / P_B[b] # P(X,Y|B) = P(X|B)P(Y|B)\n",
    "    model.addConstr(joint_prob == P_X_given_B[b, x] * P_Y_given_B[b, y], name=f'P(X,Y|B) = P(X|B)P(Y|B)')\n",
    "\n",
    "\n",
    "\n",
    "# normalization constraints:\n",
    "model.addConstr(P_A.sum() == 1, name='P_A_sum')\n",
    "model.addConstr(P_B.sum() == 1, name='P_B_sum')\n",
    "model.addConstr(P_X.sum() == 1, name='P_X_sum')\n",
    "model.addConstr(P_Y.sum() == 1, name='P_Y_sum')\n",
    "model.addConstr(P_l.sum() == 1, name='P_l_sum')\n",
    "model.addConstr(P_ABXY.sum() == 1, name='P_ABXY_sum')\n",
    "\n",
    "\n",
    "model.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy.stats import distributions\n",
    "# import networkx as nx\n",
    "\n",
    "# def sim_from_dag(dag, n_sim, sort_dag=True, check_inputs=True):\n",
    "#     if check_inputs:\n",
    "#         check_inputs_sim_from_dag(dag, n_sim, sort_dag)\n",
    "\n",
    "#     # Sample from root nodes\n",
    "#     data = {}\n",
    "#     for node in dag['root_nodes']:\n",
    "#         # Add n_sim to existing arguments\n",
    "#         args = node['params']\n",
    "#         args['size'] = n_sim\n",
    "\n",
    "#         # Call data generation function\n",
    "#         try:\n",
    "#             dist_function = getattr(distributions, node['type'])\n",
    "#             out = pd.DataFrame(dist_function(**args), columns=[node['name']])\n",
    "#         except Exception as e:\n",
    "#             raise Exception(f\"An error occurred when processing root node '{node['name']}'. The message was: {e}\")\n",
    "        \n",
    "#         data[node['name']] = out[node['name']]\n",
    "\n",
    "#     data = pd.DataFrame(data)\n",
    "\n",
    "#     if not dag['child_nodes']:\n",
    "#         return data\n",
    "\n",
    "#     # If not already ordered properly, use topological sorting to get the right data generation sequence\n",
    "#     if sort_dag:\n",
    "#         adjacency_mat = dag2matrix(dag, include_root_nodes=False)\n",
    "#         G = nx.DiGraph(adjacency_mat)\n",
    "#         index_children = list(nx.topological_sort(G))\n",
    "#     else:\n",
    "#         index_children = list(range(len(dag['child_nodes'])))\n",
    "\n",
    "#     # Go through DAG step by step\n",
    "#     for i in index_children:\n",
    "#         child = dag['child_nodes'][i]\n",
    "#         args = child.copy()\n",
    "#         args['data'] = data\n",
    "\n",
    "#         if child['type'] != \"cox\":\n",
    "#             args.pop('name', None)\n",
    "\n",
    "#         try:\n",
    "#             node_function = globals()[f\"node_{child['type']}\"]\n",
    "#             node_out = node_function(**args)\n",
    "#             data = add_node_to_data(data, node_out, child['name'])\n",
    "#         except Exception as e:\n",
    "#             raise Exception(f\"An error occurred when processing node '{child['name']}'. The message was: {e}\")\n",
    "\n",
    "#     return data\n",
    "\n",
    "# # Auxiliary function examples\n",
    "# def node_example(data, **kwargs):\n",
    "#     # Example of a function for a node processing\n",
    "#     pass\n",
    "\n",
    "# def add_node_to_data(data, new, name):\n",
    "#     # Add new node results to data\n",
    "#     data[name] = new\n",
    "#     return data\n",
    "\n",
    "# # Function to convert DAG to adjacency matrix (assuming some structure of 'dag')\n",
    "# def dag2matrix(dag, include_root_nodes):\n",
    "#     # Convert a dag to an adjacency matrix\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "import numpy as np\n",
    "\n",
    "# distribution over a dag with all observable variables, then marginalize over the latent variables to get the joint distribution as if those are latent.\n",
    "model = gp.Model(\"\")\n",
    "\n",
    "cardA, cardB, cardX, cardY, cardL = 2, 2, 2, 2, 8\n",
    "P_ABXYL = model.addMVar(shape=(cardA, cardB, cardX, cardY, cardL), vtype=gp.GRB.CONTINUOUS, lb=0, ub=1, name='P_ABXY')\n",
    "\n",
    "P_A = P_ABXY.sum(axis=(1, 2, 3))\n",
    "P_B = P_ABXY.sum(axis=(0, 2, 3))\n",
    "P_X = np.array([1/cardA for _ in range(cardA)]) # uniform distribution across setting options\n",
    "P_Y = np.array([1/cardA for _ in range(cardA)])\n",
    "# P_l is an array of cardL random values that sum to 1\n",
    "P_L = np.array([1/cardL for _ in range(cardL)])\n",
    "\n",
    "# NOT HOW THIS SHOULD BE DONE\n",
    "P_A_giv_Xl = model.addMVar(shape=(cardA, cardX, cardL), vtype=gp.GRB.CONTINUOUS, lb=0, ub=1, name='P(A|Xl)')\n",
    "P_B_giv_Yl = model.addMVar(shape=(cardB, cardY, cardL), vtype=gp.GRB.CONTINUOUS, lb=0, ub=1, name='P(B_Yl)')\n",
    "model.update()\n",
    "\n",
    "# Define necessary conditional probabilities\n",
    "P_A_given_X = np.zeros((cardA, cardX))\n",
    "P_B_given_Y = np.zeros((cardB, cardY))\n",
    "P_Y_given_X = np.zeros((cardY, cardX))\n",
    "P_X_given_Y = np.zeros((cardX, cardY))\n",
    "P_X_given_A = np.zeros((cardA, cardX))\n",
    "P_Y_given_A = np.zeros((cardA, cardY))\n",
    "P_X_given_B = np.zeros((cardB, cardX))\n",
    "P_Y_given_B = np.zeros((cardB, cardY))\n",
    "\n",
    "\n",
    "\n",
    "# for var1, var2 in np.ndindex(2,2):\n",
    "#     P_A_given_X[var1, var2] = P_ABXY[var1, :, var2, :].sum() / P_X[var2]\n",
    "#     P_B_given_Y[var1, var2] = P_ABXY[:, var1, :, var2].sum() / P_Y[var2]\n",
    "#     P_Y_given_X[var1, var2] = P_ABXY[:, :, var2, var1].sum() / P_X[var2]\n",
    "#     P_X_given_Y[var1, var2] = P_ABXY[:, :, var1, var2].sum() / P_Y[var2]\n",
    "#     P_X_given_A[var1, var2] = P_ABXY[var2, :, var1, :].sum() / P_A[var2]\n",
    "#     P_Y_given_A[var1, var2] = P_ABXY[var2, :, :, var1].sum() / P_A[var2]\n",
    "#     P_X_given_B[var1, var2] = P_ABXY[:, var2, var1, :].sum() / P_B[var2]\n",
    "#     P_Y_given_B[var1, var2] = P_ABXY[:, var2, :, var1].sum() / P_B[var2]\n",
    "def P_A_given_X(a, x):\n",
    "    return P_ABXY[a, :, x, :].sum() / P_X[x]\n",
    "def P_B_given_Y(b, y):\n",
    "    return P_ABXY[:, b, :, y].sum() / P_Y[y]\n",
    "def P_Y_given_X(y, x):\n",
    "    return P_ABXY[:, :, x, y].sum() / P_X[x]\n",
    "def P_X_given_Y(x, y):\n",
    "    return P_ABXY[:, :, x, y].sum() / P_Y[y]\n",
    "def P_X_given_A(x, a):\n",
    "    return P_ABXY[a, :, x, :].sum() / P_A[a]\n",
    "def P_Y_given_A(y, a):\n",
    "    return P_ABXY[a, :, :, y].sum() / P_A[a]\n",
    "def P_X_given_B(x, b):\n",
    "    return P_ABXY[:, b, x, :].sum() / P_B[b]\n",
    "def P_Y_given_B(y, b):\n",
    "    return P_ABXY[:, b, :, y].sum() / P_B[b]\n",
    "\"\"\"\n",
    "for x,a in np.ndindex(cardX, cardA):\n",
    "    P_A_given_X[a, x] = P_ABXY[a, :, x, :].sum() / P_X[x]\n",
    "for y,b in np.ndindex(cardY, cardB):\n",
    "    P_B_given_Y[b, y] = P_ABXY[:, b, :, y].sum() / P_Y[y]\n",
    "for x,y in np.ndindex(cardX, cardY):\n",
    "    P_Y_given_X[y, x] = P_ABXY[:, :, x, y].sum() / P_X[x]\n",
    "for y,x in np.ndindex(cardY, cardX):\n",
    "    P_X_given_Y[x, y] = P_ABXY[:, :, x, y].sum() / P_Y[y]\n",
    "for a,x in np.ndindex(cardA, cardX):\n",
    "    P_X_given_A[a, x] = P_ABXY[a, :, x, :].sum() / P_A[a]\n",
    "for a,y in np.ndindex(cardA, cardY):\n",
    "    P_Y_given_A[a, y] = P_ABXY[a, :, :, y].sum() / P_A[a]\n",
    "for b,x in np.ndindex(cardB, cardX):\n",
    "    P_X_given_B[b, x] = P_ABXY[:, b, x, :].sum() / P_B[b]\n",
    "for b,y in np.ndindex(cardB, cardY):\n",
    "    P_Y_given_B[b, y] = P_ABXY[:, b, :, y].sum() / P_B[b]\n",
    "\"\"\"\n",
    "\n",
    "# SEM\n",
    "for a,b,x,y in np.ndindex(cardA, cardB, cardX, cardY):\n",
    "    # model.addConstr(P_AB_giv_XY[a,b,x,y] == [P_A_giv_Xl[a,x,l] * P_B_giv_Yl(b,y,l)*P_l[l] for l in range(cardL)].sum())\n",
    "    model.addConstr(P_ABXY[a,b,x,y] == sum([P_A_giv_Xl[a,x,l] * P_B_giv_Yl[b,y,l]*P_X[x]*P_Y[y]*P_l[l] for l in range(cardL)]))\n",
    "\n",
    "\n",
    "# conditional independence constraints for bell DAG\n",
    "\n",
    "# X ⫫ Y \n",
    "# -> P(X, Y) = P(X)P(Y)\n",
    "for x,y in np.ndindex(cardX, cardY):\n",
    "    model.addConstr(P_ABXY[:, :, x, y].sum() == P_X[x] * P_Y[y], name='X⫫Y')\n",
    "    \n",
    "# A ⫫ Y | X \n",
    "# -> P(A, Y | X) = P(A | X)P(Y | X)\n",
    "for a,x,y in np.ndindex(cardA, cardX, cardY):\n",
    "    model.addConstr(\n",
    "        # P(A,X,Y)/P(X) = P(A,Y|X) = P(A|X)P(Y|X)\n",
    "        P_ABXY[a, :, x, y].sum() / P_X[x] == P_A_given_X[a, x] * P_Y_given_X[y, x], name=f'P(A,Y|X)=P(A|X)P(Y|X)')\n",
    "\n",
    "# B ⫫ X | Y \n",
    "# -> P(B, X | Y) = P(B | Y)P(X | Y)\n",
    "for b,x,y in np.ndindex(cardB,cardX,cardY):\n",
    "    model.addConstr(\n",
    "        # P(B,X,Y)/P(Y) = P(B,X|Y) = P(B|Y)P(X|Y)\n",
    "        P_ABXY[:, b, x, y].sum() / P_Y[y] == P_B_given_Y[b, y] * P_X_given_Y[x, y], name=f'P(B,X|Y)=P(B|Y)P(X|Y)')\n",
    "\n",
    "# A ⫫ Y \n",
    "# -> P(A, Y) = P(A)P(Y)\n",
    "for a,y in np.ndindex(cardA, cardY):\n",
    "    model.addConstr(P_ABXY[a, :, :, y].sum() == P_A[a] * P_Y[y], name=f'A⫫Y_{a}_{y}')\n",
    "\n",
    "# B ⫫ X \n",
    "# -> P(B,X) = P(B)P(X)\n",
    "for b,x in np.ndindex(cardB, cardX):\n",
    "    model.addConstr(P_ABXY[:, b, x, :].sum() == P_B[b] * P_X[x], name=f'B⫫X_{b}_{x}')\n",
    "\n",
    "for a,x,y in np.ndindex(cardA, cardX, cardY):\n",
    "    joint_prob = P_ABXY[a, :, x, y].sum() / P_A[a] # P(X,Y|A) = P(X|A)P(Y|A)\n",
    "    model.addConstr(joint_prob == P_X_given_A[a, x] * P_Y_given_A[a, y], name=f'P(X,Y|A) = P(X|A)P(Y|A)')\n",
    "\n",
    "# X ⫫ Y | B -> P(X, Y | B) = P(X | B)P(Y | B) and = P(X)P(Y | B)\n",
    "for b,x,y in np.ndindex(cardB, cardX, cardY):\n",
    "    joint_prob = P_ABXY[:, b, x, y].sum() / P_B[b] # P(X,Y|B) = P(X|B)P(Y|B)\n",
    "    model.addConstr(joint_prob == P_X_given_B[b, x] * P_Y_given_B[b, y], name=f'P(X,Y|B) = P(X|B)P(Y|B)')\n",
    "\n",
    "\n",
    "\n",
    "# normalization constraints:\n",
    "model.addConstr(P_A.sum() == 1, name='P_A_sum')\n",
    "model.addConstr(P_B.sum() == 1, name='P_B_sum')\n",
    "model.addConstr(P_X.sum() == 1, name='P_X_sum')\n",
    "model.addConstr(P_Y.sum() == 1, name='P_Y_sum')\n",
    "model.addConstr(P_l.sum() == 1, name='P_l_sum')\n",
    "model.addConstr(P_ABXY.sum() == 1, name='P_ABXY_sum')\n",
    "\n",
    "\n",
    "model.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(P_l)\n",
    "P_A = P_ABXYl.sum(axis=(1, 2, 3, 4))\n",
    "P_B = P_ABXYl.sum(axis=(0, 2, 3, 4))\n",
    "\n",
    "\n",
    "P_A_giv_X = gp.tupledict()\n",
    "P_B_giv_Y = gp.tupledict()\n",
    "P_Y_giv_X = gp.tupledict()\n",
    "P_X_giv_Y = gp.tupledict()\n",
    "P_A_giv_Xl = gp.tupledict()\n",
    "P_B_giv_Yl = gp.tupledict()\n",
    "for var1, var2 in np.ndindex(2,2):\n",
    "    P_A_giv_X[var1, var2] = P_ABXYl[var1,:,var2,:,:].sum() / P_X[var2]\n",
    "    P_B_giv_Y[var1, var2] = P_ABXYl[:,var1,:,var2,:].sum() / P_Y[var2]\n",
    "    P_Y_giv_X[var1, var2] = P_ABXYl[:,:,var2,var1,:].sum() / P_X[var2]\n",
    "    P_X_giv_Y[var1, var2] = P_ABXYl[:,:,var1,var2,:].sum() / P_Y[var2]\n",
    "\n",
    "for a,x,l in np.ndindex(cardA, cardX, cardL): # P(A|X,l) from P(A,B,X,Y,l)\n",
    "    P_A_giv_Xl[a,x,l] = P_ABXYl[a,:,x,:,l].sum() / (P_X[x] * P_l[l]) # P(A|X)/ (P(x)*P(l))\n",
    "for b,y,l in np.ndindex(cardB, cardY, cardL): # P(B|Y,l) from P(A,B,X,Y,l)\n",
    "    P_B_giv_Yl[b,y,l] = P_ABXYl[:,b,:,y,l].sum() / (P_Y[y] * P_l[l]) # P(B|Y)/ (P(B)*P(l))\n",
    "\n",
    "# P(X)*P(L) = P(X,L)\n",
    "for x,l in np.ndindex(cardX, cardL):\n",
    "    model.addConstr(P_ABXYl[:,:,x,:,l].sum() == P_ABXYl[:,:,x,:,:].sum() * P_ABXYl[:,:,:,:,l].sum(), name='P(X)*P(L) = P(X,L)')\n",
    "# P(Y)*P(L) = P(Y,L)\n",
    "for y,l in np.ndindex(cardY, cardL):\n",
    "    model.addConstr(P_ABXYl[:,:,:,y,l].sum() == P_ABXYl[:,:,:,y,:].sum() * P_ABXYl[:,:,:,:,l].sum() , name='P(Y)*P(L) = P(Y,L)')\n",
    "\n",
    "## SEM\n",
    "# for a,b,x,y in np.ndindex(cardA, cardB, cardX, cardY):\n",
    "#     model.addConstr(P_ABXYl[a,b,x,y] == sum([P_A_giv_Xl[a,x,l] * P_B_giv_Yl[b,y,l]*P_X[x]*P_Y[y]*P_l[l] for l in range(cardL)]))\n",
    "for a,b,x,y,l in np.ndindex(*P_ABXYl.shape):\n",
    "    P_ABXYl[a,b,x,y,l] == P_A_giv_Xl[a,x,l] * P_B_giv_Yl[b,y,l]*P_X[x]*P_Y[y]*P_l[l]\n",
    "\n",
    "\n",
    "##### conditional independence constraints for bell DAG\n",
    "# X ⫫ Y \n",
    "# -> P(X, Y) = P(X)P(Y)\n",
    "for x,y in np.ndindex(cardX, cardY):\n",
    "    model.addConstr(P_ABXYl[:, :, x, y,:].sum() == P_X[x] * P_Y[y], name='X⫫Y')\n",
    "    \n",
    "# A ⫫ Y | X \n",
    "# -> P(A, Y | X) = P(A | X)P(Y | X)\n",
    "for a,x,y in np.ndindex(cardA, cardX, cardY):\n",
    "    model.addConstr(\n",
    "        # P(A,X,Y)/P(X) = P(A,Y|X) = P(A|X)P(Y|X)\n",
    "        P_ABXYl[a, :, x, y,:].sum() / P_X[x] == P_A_giv_X[a, x] * P_Y_giv_X[y, x], name=f'P(A,Y|X)=P(A|X)P(Y|X)')\n",
    "\n",
    "# B ⫫ X | Y \n",
    "# -> P(B, X | Y) = P(B | Y)P(X | Y)\n",
    "for b,x,y in np.ndindex(cardB,cardX,cardY):\n",
    "    model.addConstr(\n",
    "        # P(B,X,Y)/P(Y) = P(B,X|Y) = P(B|Y)P(X|Y)\n",
    "        P_ABXYl[:, b, x, y,:].sum() / P_Y[y] == P_B_giv_Y[b, y] * P_X_giv_Y[x, y], name=f'P(B,X|Y)=P(B|Y)P(X|Y)')\n",
    "\n",
    "# A ⫫ Y \n",
    "# -> P(A, Y) = P(A)P(Y)\n",
    "for a,y in np.ndindex(cardA, cardY):\n",
    "    model.addConstr(P_ABXYl[a, :, :, y,:].sum() == P_A[a] * P_Y[y], name=f'A⫫Y_{a}_{y}')\n",
    "\n",
    "# B ⫫ X \n",
    "# -> P(B,X) = P(B)P(X)\n",
    "for b,x in np.ndindex(cardB, cardX):\n",
    "    model.addConstr(P_ABXYl[:, b, x, :,:].sum() == P_B[b] * P_X[x], name=f'B⫫X_{b}_{x}')\n",
    "\n",
    "# for a,x,y in np.ndindex(cardA, cardX, cardY):\n",
    "#     joint_prob = P_ABXYl[a, :, x, y,:].sum() / P_A[a] # P(X,Y|A) = P(X|A)P(Y|A)\n",
    "#     model.addConstr(joint_prob == P_X_giv_A[a, x] * P_Y_giv_A[a, y], name=f'P(X,Y|A) = P(X|A)P(Y|A)')\n",
    "\n",
    "# # X ⫫ Y | B -> P(X, Y | B) = P(X | B)P(Y | B) and = P(X)P(Y | B)\n",
    "# for b,x,y in np.ndindex(cardB, cardX, cardY):\n",
    "#     joint_prob = P_ABXYl[:, b, x, y,:].sum() / P_B[b] # P(X,Y|B) = P(X|B)P(Y|B)\n",
    "#     model.addConstr(joint_prob == P_X_giv_B[b, x] * P_Y_giv_B[b, y], name=f'P(X,Y|B) = P(X|B)P(Y|B)')\n",
    "\n",
    "\n",
    "# normalization constraints:\n",
    "model.addConstr(P_A.sum() == 1, name='P_A_sum')\n",
    "model.addConstr(P_B.sum() == 1, name='P_B_sum')\n",
    "model.addConstr(P_ABXYl.sum() == 1, name='P_ABXYl_sum')\n",
    "## need to write more generally:\n",
    "for j in range(2):\n",
    "    model.addConstr(sum([P_A_giv_X[i,j] for i in range(2)])==1)\n",
    "    model.addConstr(sum([P_B_giv_Y[i,j] for i in range(2)])==1)\n",
    "    model.addConstr(sum([P_Y_giv_X[i,j] for i in range(2)])==1)\n",
    "    model.addConstr(sum([P_X_giv_Y[i,j] for i in range(2)])==1)\n",
    "for j,k in np.ndindex(2,2):\n",
    "    model.addConstr(sum([P_A_giv_Xl[i,j,k] for i in range(2)])==1)\n",
    "    model.addConstr(sum([P_B_giv_Yl[i,j,k] for i in range(2)])==1)  \n",
    "##\n",
    "\n",
    "\n",
    "model.optimize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
